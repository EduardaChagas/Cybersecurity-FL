{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Flatten, Dense, Reshape\n",
    "from keras.layers import Input,InputLayer, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import losses\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import keras\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import scipy.io\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import preprocessing\n",
    "import scipy.io\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tensorflow import keras\n",
    "from keras.layers import Conv2D,MaxPool2D,Dense,Dropout,Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import optimizers\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from keras.layers import AveragePooling2D, Input, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.datasets import cifar10\n",
    "from keras import losses\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_print(op, tensors, message=\"\"):\n",
    "    def print_message(x):\n",
    "        sys.stdout.write(\"\\n DEBUG: \" + message + \" %s\\n\" % x)\n",
    "        return x\n",
    "\n",
    "    prints = [tf.compat.v1.py_func(print_message, [tensor], tensor.dtype) for tensor in tensors]\n",
    "    with tf.control_dependencies(prints):\n",
    "        op = tf.identity(op)\n",
    "    return op\n",
    "\n",
    "def tf_print_2(tensor, tensors):\n",
    "    def print_message(x):\n",
    "        message = \"\"\n",
    "        sys.stdout.write(\"DEBUG: \" + message + \" %s\" % x)\n",
    "        return x\n",
    "\n",
    "    prints = [tf.compat.v1.py_func(print_message, [tensors], tensor.dtype)]\n",
    "    with tf.control_dependencies(prints):\n",
    "        tensor = tf.identity(tensor)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def pairwise_dist(A):\n",
    "    # Taken frmo https://stackoverflow.com/questions/37009647/compute-pairwise-distance-in-a-batch-without-replicating-tensor-in-tensorflow\n",
    "    r = tf.reduce_sum(A*A, 1)\n",
    "    r = tf.reshape(r, [-1, 1])\n",
    "    D = tf.maximum(r - 2*tf.matmul(A, tf.transpose(A)) + tf.transpose(r), 1e-7)\n",
    "    D = tf.sqrt(D)\n",
    "    return D\n",
    "\n",
    "def dist_corr(X, Y):\n",
    "    n = tf.cast(tf.shape(X)[0], tf.float32)\n",
    "    a = pairwise_dist(X)\n",
    "    b = pairwise_dist(Y)\n",
    "    A = a - tf.reduce_mean(a, axis=1) - tf.expand_dims(tf.reduce_mean(a, axis=0), axis=1) + tf.reduce_mean(a)\n",
    "    B = b - tf.reduce_mean(b, axis=1) - tf.expand_dims(tf.reduce_mean(b, axis=0), axis=1) + tf.reduce_mean(b)\n",
    "    dCovXY = tf.sqrt(tf.reduce_sum(A*B) / (n ** 2))\n",
    "    dVarXX = tf.sqrt(tf.reduce_sum(A*A) / (n ** 2))\n",
    "    dVarYY = tf.sqrt(tf.reduce_sum(B*B) / (n ** 2))\n",
    "    \n",
    "    dCorXY = dCovXY / tf.sqrt(dVarXX * dVarYY)\n",
    "    return dCorXY\n",
    "\n",
    "def custom_loss1(y_true,y_pred):\n",
    "    dcor = dist_corr(y_true,y_pred)\n",
    "    return dcor\n",
    "\n",
    "def custom_loss2(y_true,y_pred):\n",
    "    recon_loss = losses.categorical_crossentropy(y_true, y_pred)\n",
    "    return recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha1, alpha2 = 1000., 0.1\n",
    "stage_num, block_num = 2, 1\n",
    "experiment_name = \"cifar10_{}_{}_{}_{}\".format(alpha1, alpha2, stage_num, block_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 256]\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "batch_size = 128  # orig paper trained all networks with batch_size=128\n",
    "epochs = 10\n",
    "data_augmentation = False\n",
    "num_classes = 10\n",
    "\n",
    "# Subtracting pixel mean improves accuracy\n",
    "subtract_pixel_mean = False\n",
    "\n",
    "n = 3\n",
    "\n",
    "# Computed depth from supplied model parameter n\n",
    "depth = n * 9 + 2\n",
    "\n",
    "# Load the CIFAR10 data.\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_trainRaw = x_train\n",
    "x_testRaw = x_test\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# If subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-4\n",
    "    if epoch > 180:\n",
    "        lr = 1e-6\n",
    "    elif epoch > 160:\n",
    "        lr = 7e-6\n",
    "    elif epoch > 120:\n",
    "        lr = 2e-5\n",
    "    elif epoch > 80:\n",
    "        lr = 8e-5\n",
    "    return lr\n",
    "\n",
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "    \n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "    Features maps sizes:\n",
    "    conv1  : 32x32,  16\n",
    "    stage 0: 32x32,  64\n",
    "    stage 1: 16x16, 128\n",
    "    stage 2:  8x8,  256\n",
    "\n",
    "    # Arguments\n",
    "        input_shape (tensor): shape of input image tensor\n",
    "        depth (int): number of core convolutional layers\n",
    "        num_classes (int): number of classes (CIFAR10 has 10)\n",
    "\n",
    "    # Returns\n",
    "        model (Model): Keras model instance\n",
    "    \"\"\"\n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
    "    # Start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            if stage_num == stage and block_num == res_block:\n",
    "                before_flatten_dims = x.get_shape().as_list()[1:]\n",
    "                split_layer = Flatten(name='split_layer')\n",
    "                split_layer_output = split_layer(x)\n",
    "                x = Reshape(before_flatten_dims)(split_layer_output)\n",
    "                print(before_flatten_dims)\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    x = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    name='softmax')(x)\n",
    "\n",
    "    # Instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=[split_layer_output, outputs])\n",
    "    #model = Model(inputs=inputs, outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = resnet_v2(input_shape=input_shape, depth=depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:AutoGraph could not transform <function dist_corr at 0x7f42c8ef5310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function dist_corr at 0x7f42c8ef5310> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss={'split_layer': custom_loss1, 'softmax': custom_loss2},\n",
    "              loss_weights={'split_layer': alpha1, 'softmax': alpha2},\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics={'softmax':'accuracy'})\n",
    "#model.compile(loss=custom_loss2, optimizer=Adam(lr=lr_schedule(0)),\n",
    "#              metrics=['accuracy'])\n",
    "#model.summary()\n",
    "#print(model_type)\n",
    "\n",
    "# Prepare model model saving directory.\n",
    "save_dir = os.path.join(os.getcwd(), '../saved_models')\n",
    "model_name = '%s_model.{epoch:03d}.h5' % experiment_name\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)\n",
    "\n",
    "# Prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_loss',\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=1e-5)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]\n",
    "\n",
    "# Run training, with or without data augmentation.\n",
    "x_train_flattened = x_train.reshape(50000, 32*32*3)\n",
    "x_test_flattened = x_test.reshape(10000, 32*32*3)\n",
    "\n",
    "history = model.fit(x_train, [x_train_flattened, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, [x_test_flattened, y_test]),\n",
    "          shuffle=True,\n",
    "          callbacks=callbacks,\n",
    "          verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, [x_train_flattened, y_train],\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, [x_test_flattened, y_test]),\n",
    "          shuffle=True,\n",
    "          callbacks=callbacks,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  plot_train_history(history):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(25, 6))\n",
    "    \n",
    "    axes[0].plot(history.history['split_layer_loss'], label='Split Layer loss')\n",
    "    axes[0].plot(history.history['val_split_layer_loss'], label='Split Val loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    axes[1].plot(history.history['softmax_loss'], label='Softmax loss')\n",
    "    axes[1].plot(history.history['val_softmax_loss'], label='Validation softmax loss')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(history.history['softmax_accuracy'], label='Softmax acc')\n",
    "    axes[2].plot(history.history['val_softmax_accuracy'], label='Validation softmax acc')\n",
    "    axes[2].set_xlabel('Epochs')\n",
    "    axes[2].legend()\n",
    "\n",
    "plot_train_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test raw vs smash\n",
    "n = 20\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(10,20):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i])\n",
    "    #plt.imshow((x_test[i] * 255).astype(np.int64))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(x_test_encoded[0][i].reshape(16, 16, 3))\n",
    "    #plt.imshow((x_test_encoded[0][i].reshape(32, 32, 3) * 255).astype(np.int64))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/tf/datasets/{}/output/'.format(experiment_name)\n",
    "inp_dir = '/tf/datasets/{}/input/'.format(experiment_name)\n",
    "os.makedirs(out_dir)\n",
    "os.makedirs(inp_dir)\n",
    "import matplotlib\n",
    "for i in range(10000):\n",
    "    #np.save('rawCifar10_baseline/'+str(i), x_test[i],allow_pickle = True)\n",
    "    #np.save('noSmashCifar10_baseline/'+str(i), x_test_encoded[0][i].reshape(32, 32, 3),allow_pickle = True)\n",
    "    np.save('{}/{}'.format(out_dir, i), x_test_encoded[0][i].reshape(8, 8, 256),allow_pickle = True)\n",
    "    np.save('{}/{}'.format(inp_dir, i), x_testRaw[i].reshape(32, 32, 3),allow_pickle = True)\n",
    "    \n",
    "    #matplotlib.image.imsave('rawCifar10/'+str(i)+'.png', x_test[i])\n",
    "    #matplotlib.image.imsave('smashCifar10/'+str(i)+'.png', x_test_encoded[0][i].reshape(32, 32, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('/tf/datasets/{}/trainHistoryDict'.format(experiment_name), 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "with open('/tf/datasets/{}/trainHistoryDict2'.format(experiment_name), 'wb') as file_pi:\n",
    "    pickle.dump(history2.history, file_pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train raw vs smash\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1,n):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_train[i])\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(x_train_encoded[0][i].reshape(32, 32, 3))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
